{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee66a6f6",
   "metadata": {},
   "source": [
    "# 데이터 다운로드 하기\n",
    ": 인터넷에서 지정된 파일을 내 PC에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c859c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# URL과 저장경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "print(\"저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d377df0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# URL과 저장경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test1.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "#파일로 저장하기(바이너리, 텍스트 다름)\n",
    "with open(savename, \"wb\") as f:\n",
    "    f.write(mem)\n",
    "print(\"저장되었습니다.\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41984f72",
   "metadata": {},
   "source": [
    "# BeautifulSoup로 Scraping하기\n",
    ": 간단하게 HTML과 XML에서 정보를 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a8e8c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/imhyeonjin/opt/anaconda3/lib/python3.8/site-packages (4.9.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/imhyeonjin/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d121b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "<body>\n",
      "<h1>Header</h1>\n",
      "<p>Line 1을 서술</p>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "<h1>Header</h1>\n",
      "<p>Line 1을 서술</p>\n",
      "h1= Header\n",
      "h1= Header\n",
      "p1= Line 1을 서술\n",
      "p1= Line 1을 서술\n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <h1>Header</h1>\n",
    "                <p>Line 1을 서술</p>\n",
    "            </body>\n",
    "        </html>    \n",
    "        \n",
    "\"\"\"\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "print(soup)\n",
    "\n",
    "\n",
    "#원하는 부분 추출하기\n",
    "h1 = soup. html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "print(p1)\n",
    "\n",
    "#Text만 출력\n",
    "print(\"h1 =\",h1.string)\n",
    "print(\"h1 =\",h1.text)\n",
    "print(\"p1 =\",p1.string)\n",
    "print(\"p1 =\",p1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b99200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">Header</h1>\n",
      "<p id=\"body\">Line 1을 서술</p>\n",
      "title= Header\n",
      "body= Line 1을 서술\n"
     ]
    }
   ],
   "source": [
    "# id로 요소를 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Html Sample\n",
    "\n",
    "html = \"\"\"\n",
    "     <html>\n",
    "            <body>\n",
    "                <h1 id=\"title\">Header</h1>\n",
    "                <p id=\"body\">Line 1을 서술</p>\n",
    "            </body>\n",
    "        </html>    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "#원하는 부분 추출하기\n",
    "title = soup.find(id='title')\n",
    "print(title)\n",
    "body = soup.find(id='body')\n",
    "print(body)\n",
    "\n",
    "#text만 출력\n",
    "print(\"title =\",title.string)\n",
    "print(\"body =\",body.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "356715a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver >>> http://www.naver.com\n",
      "daum >>> http://www.daum.com\n"
     ]
    }
   ],
   "source": [
    "#여러 개의 요소 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "     <html>\n",
    "            <body>\n",
    "                <ul>\n",
    "                    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "                    <li><a href=\"http://www.daum.com\">daum</a></li>\n",
    "                </ul>\n",
    "            </body>\n",
    "        </html>    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#HTML 분석\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "#원하는 부분 호출하기\n",
    "links = soup.find_all('a')\n",
    "\n",
    "#링크 목록 출력하기\n",
    "for link in links:\n",
    "#     print(link.string)\n",
    "#     print(link.attrs['href'])\n",
    "    href = link.attrs['href']\n",
    "    text = link.string\n",
    "    print(text,\">>>\",href)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb85bde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------\n",
      " ○ (강수) 16일(금) 오후에는 소나기가 내리는 곳이 있겠고, 19일(월) 오후에는 비가 내리겠습니다. ○ (기온) 이번 예보기간 아침최저기온은 23~26도, 낮최고기온은 29~34도로 어제(12일, 아침최저기온 20~24도, 낮최고기온 30~33도)와 비슷하거나 조금 높겠습니다. ○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다. ○ (주말전망) 17일(토)~18일(일)은 구름많겠고, 아침 기온은 23~24도, 낮 기온은 29~33도가 되겠습니다  * 이번 예보기간 동안 내륙에는 낮최고기온이 33도 내외, 아침최저기온이 25도 내외로 오르는 곳이 많아 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다. * 또한, 북태평양고기압 위치에 따른 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' (강수) 16일(금) 오후에는 소나기가 내리는 곳이 있겠고, 19일(월) 오후에는 비가 내리겠습니다. ',\n",
       " ' (기온) 이번 예보기간 아침최저기온은 23~26도, 낮최고기온은 29~34도로 어제(12일, 아침최저기온 20~24도, 낮최고기온 30~33도)와 비슷하거나 조금 높겠습니다. ',\n",
       " ' (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다. ',\n",
       " ' (주말전망) 17일(토)~18일(일)은 구름많겠고, 아침 기온은 23~24도, 낮 기온은 29~33도가 되겠습니다  * 이번 예보기간 동안 내륙에는 낮최고기온이 33도 내외, 아침최저기온이 25도 내외로 오르는 곳이 많아 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다. * 또한, 북태평양고기압 위치에 따른 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기상청 자료 활용하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "#data 가져오기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "#print(soup)\n",
    "\n",
    "#원하는 데이터 추출하기\n",
    "title = soup.find('title').string\n",
    "#print(title)\n",
    "wf = soup.find('wf').string\n",
    "# type(wf)\n",
    "# list(wf)\n",
    "# w = str(wf)\n",
    "# list1 = w.split('<br />')\n",
    "# print(list1[0])\n",
    "# for i in list1:\n",
    "#     print(i)\n",
    "\n",
    "list_wfs = list(wf)\n",
    "except_char = ['<','b','r','/','>']\n",
    "result = \" \"\n",
    "\n",
    "for lwf in list_wfs:\n",
    "    if lwf not in except_char:\n",
    "        result += lwf\n",
    "        \n",
    "print('-'* 115)\n",
    "print(result)\n",
    "type(result)\n",
    "result.split('○')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c64ef",
   "metadata": {},
   "source": [
    "# CSS 선택자 사용하기\n",
    "- soup.select_one(): css선택자로 요소 하나를 추출\n",
    "- soup.select(): css선택자로 요소 여러개를 리스트로 추출    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "729a0a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위키북스\n",
      "유니티 게임 이펙트 입문서\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"\"\"\n",
    "     <html>\n",
    "            <body>\n",
    "                <div id = \"meigen\">\n",
    "                    <h1>위키북스</h1>\n",
    "                    <ul class=\"items\">\n",
    "                    <li>유니티 게임 이펙트 입문서</li>\n",
    "                    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "                    <li>모던 웹사이트 디자인의 정석</li>\n",
    "                </ul>\n",
    "            </body>\n",
    "        </html>    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#필요한 부분 css로 추출하기\n",
    "#타이틀 부분 추출하기\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string # 찾을 때 id:#, class:.으로 시작한다 > : 자손, \" \": 후손\n",
    "print(h1)\n",
    "\n",
    "#목록 부분 추출하기\n",
    "li_lists = soup.select(\"div#meigen > ul.items > li\")\n",
    "#li_lists = soup.select(\"div#meigen li\") 후손이 아니기 때문에 >를 쓰면 안된다.\n",
    "#print(li_lists)\n",
    "\n",
    "for li in li_lists:\n",
    "    print(li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b975ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 금융에서 환율 정보 추출하기\n",
    "#https://finance.naver.com/marketindex/\n",
    "#exchangeList >li.on>a.head.usd>div>span.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "172afb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/ krw :  1,143.10\n"
     ]
    }
   ],
   "source": [
    "#미국 환율 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "#HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "#HTML 분석\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "\n",
    "#데이터 추출하기\n",
    "price = soup.select_one(\"span.value\").string\n",
    "print(\"usd/ krw : \", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d8a0d9b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 :  1,143.90\n",
      "일본 :  1,036.47\n",
      "유럽연합 :  1,357.81\n",
      "중국 :  176.84\n",
      "1,036.47\n",
      "1,357.81\n",
      "176.84\n",
      "110.2700\n"
     ]
    }
   ],
   "source": [
    "#미국, 일본, 유럽연합, 중국의 환율\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "#HTML\n",
    "url=\"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "#HTML 분석\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "\n",
    "#데이터 추출하기\n",
    "prices = soup.select(\"div.head_info>span.value\")\n",
    "print(\"미국 : \", prices[0].string)\n",
    "print(\"일본 : \", prices[1].string)\n",
    "print(\"유럽연합 : \", prices[2].string)\n",
    "print(\"중국 : \", prices[3].string)\n",
    "\n",
    "for i in range(1,5):\n",
    "    print(prices[i].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cf339011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘과 바람과 별과 시\n",
      "서시\n",
      "자화상\n",
      "소년\n",
      "눈 오는 지도\n",
      "돌아와 보는 밤\n",
      "병원\n",
      "새로운 길\n",
      "간판 없는 거리\n",
      "태초의 아침\n",
      "또 태초의 아침\n",
      "새벽이 올 때까지\n",
      "무서운 시간\n",
      "십자가\n",
      "바람이 불어\n",
      "슬픈 족속\n",
      "눈감고 간다\n",
      "또 다른 고향\n",
      "길\n",
      "별 헤는 밤\n",
      "흰 그림자\n",
      "사랑스런 추억\n",
      "흐르는 거리\n",
      "쉽게 씌어진 시\n",
      "봄\n",
      "참회록\n",
      "간(肝)\n",
      "위로\n",
      "팔복\n",
      "못자는밤\n",
      "달같이\n",
      "고추밭\n",
      "아우의 인상화\n",
      "사랑의 전당\n",
      "이적\n",
      "비오는 밤\n",
      "산골물\n",
      "유언\n",
      "창\n",
      "바다\n",
      "비로봉\n",
      "산협의 오후\n",
      "명상\n",
      "소낙비\n",
      "한난계\n",
      "풍경\n",
      "달밤\n",
      "장\n",
      "밤\n",
      "황혼이 바다가 되어\n",
      "아침\n",
      "빨래\n",
      "꿈은 깨어지고\n",
      "산림\n",
      "이런날\n",
      "산상\n",
      "양지쪽\n",
      "닭\n",
      "가슴 1\n",
      "가슴 2\n",
      "비둘기\n",
      "황혼\n",
      "남쪽 하늘\n",
      "창공\n",
      "거리에서\n",
      "삶과 죽음\n",
      "초한대\n",
      "산울림\n",
      "해바라기 얼굴\n",
      "귀뚜라미와 나와\n",
      "애기의 새벽\n",
      "햇빛·바람\n",
      "반디불\n",
      "둘 다\n",
      "거짓부리\n",
      "눈\n",
      "참새\n",
      "버선본\n",
      "편지\n",
      "봄\n",
      "무얼 먹구 사나\n",
      "굴뚝\n",
      "햇비\n",
      "빗자루\n",
      "기왓장 내외\n",
      "오줌싸개 지도\n",
      "병아리\n",
      "조개껍질\n",
      "겨울\n",
      "트루게네프의 언덕\n",
      "달을 쏘다\n",
      "별똥 떨어진 데\n",
      "화원에 꽃이 핀다\n",
      "종시\n"
     ]
    }
   ],
   "source": [
    "# 윤동주 시안 작품 가져오기\n",
    "#https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\n",
    "# #mw-content-text > div.mw-parser-output > ul:nth-child(6) > li > b > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "poetry = soup.select(\"div#mw-content-text >div.mw-parser-output  ul  li a\")\n",
    "# for j in range(1,80):\n",
    "#     print(poetry[j].text)\n",
    "for j in poetry:\n",
    "    if j.string=='증보판':\n",
    "        continue\n",
    "    print(j.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed1814c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "남산의 부장들\n",
      "다만 악에서 구하소서\n",
      "반도\n",
      "히트맨\n",
      "테넷\n",
      "백두산\n",
      "#살아있다\n",
      "강철비2: 정상회담\n",
      "담보\n",
      "닥터 두리틀\n",
      "삼진그룹 영어토익반\n",
      "정직한 후보\n",
      "도굴\n",
      "클로젯\n",
      "오케이 마담\n",
      "해치지않아\n",
      "천문: 하늘에 묻는다\n",
      "결백\n",
      "1917\n",
      "작은 아씨들\n",
      "미드웨이\n",
      "시동\n",
      "지푸라기라도 잡고 싶은 짐승들\n",
      "미스터 주: 사라진 VIP\n",
      "인비저블맨\n",
      "나쁜 녀석들: 포에버\n",
      "국제수사\n",
      "침입자\n",
      "스타워즈: 라이즈 오브 스카이워커\n",
      "스파이 지니어스 \n",
      "이웃사촌\n",
      "온워드: 단 하루의 기적\n",
      "소리도 없이\n",
      "버즈 오브 프레이(할리 퀸의 황홀한 해방)\n",
      "원더 우먼 1984\n",
      "겨울왕국 2\n",
      "오! 문희\n",
      "그린랜드\n",
      "위대한 쇼맨\n",
      "런\n",
      "뮬란\n",
      "내가 죽던 날\n",
      "기생충\n",
      "신비아파트 극장판 하늘도깨비 대 요르문간드\n",
      "프리즌 이스케이프\n",
      "검객\n",
      "조제\n",
      "사라진 시간\n",
      "밤쉘: 세상을 바꾼 폭탄선언\n",
      "알라딘\n"
     ]
    }
   ],
   "source": [
    "# 다음 영화 연간 순위 가져오기\n",
    "#https://movie.daum.net/boxoffice/yearly?year=2021\n",
    "# #mainContent > div > div.box_boxoffice > ol > li > div > div.thumb_cont > strong\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://movie.daum.net/boxoffice/yearly?year=2021\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "# movie = soup.select(\"#mainContent > div > div.box_boxoffice > ol > li > div > div.thumb_cont>strong\")\n",
    "movie = soup.select(\"strong a\")\n",
    "for i in movie:\n",
    "    print(i.string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6483a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://v.daum.net/v/20210713144711324 : \"일출 보려다 물어뜯길 뻔\"..성산일출봉까지 점령한 야생들개\n",
      "https://v.daum.net/v/20210713155234023 : [영상] \"하늘에서 물고기가 비처럼 내려와\" 무슨 일?\n",
      "https://v.daum.net/v/20210713140143407 : 이재명, 긴급 기자회견..\"4차 팬데믹 못막으면 전면봉쇄 불가피\"\n",
      "https://v.daum.net/v/20210713152051730 : 55억 '남산 뷰' 이태원집 매입한 박나래..5년간 200억 넘게 벌었다\n",
      "https://v.daum.net/v/20210713151947679 : 전여옥, 위기의 이준석에 \"뒤에서 칼꽂는 게 정치..성인식 치러\"\n",
      "https://v.daum.net/v/20210713154045534 : 집주인 허락없이 페인트 칠한 세입자의 만행..손배 가능할까\n",
      "https://v.daum.net/v/20210713144622300 : \"확진자인데 와서 미안\" 농담에 카페 영업중단..업무방해 무죄\n",
      "https://v.daum.net/v/20210713155317055 : 남아공 폭동에 LG공장 전소·약탈..\"길도 다닐 수 없는 지경\"\n",
      "https://v.daum.net/v/20210713154215620 : 구미 친모 \"낳지도, 바꾸지도 않았다\"..검찰 징역 13년 구형\n",
      "https://v.daum.net/v/20210713150601168 : [영상]\"철학 붕괴\" 윤희숙 연일 이준석 비판..하태경 \"자해정치\"[이슈시개]\n",
      "https://v.daum.net/v/20210713135902248 : \"이 시국에 떼창?\" 팬들 반발에..백기 든 '미스터트롯' 공연\n",
      "https://v.daum.net/v/20210713161800990 : \"성매매업소 방이 43개, 처음 현장 보니 가슴 먹먹\"\n",
      "https://v.daum.net/v/20210713160104350 : \"친구 언니가 무서웠다\"..전북에서 찾은 대전 실종 초등생\n",
      "https://v.daum.net/v/20210713154045535 : 화이자 백신 79만9천회분 내일 도착..고3·교직원 접종에 사용\n",
      "https://v.daum.net/v/20210713161802995 : 윤석열의 '가족 리스크'..김건희, 이번엔 '공소시효 언급' 자충수\n",
      "https://v.daum.net/v/20210713152355845 : 중국서 2명 살해한 50대 현지인..34년만에 인천서 검거\n",
      "https://v.daum.net/v/20210713132549158 : BJ철구, 7살딸과 '여캠BJ 새엄마 월드컵'..아동학대 논란\n",
      "https://v.daum.net/v/20210713120602280 : 습도 높으면 35도서도 치명타..습한 한국 여름 무서운 이유\n",
      "https://v.daum.net/v/20210713124446148 : \"누나가 무슨 부모야\" 친누나 30차례 찔러 살해한 남동생.. 무기징역 구형\n",
      "https://v.daum.net/v/20210713152516902 : 울산서 숲체험 유치원생 24명에 '벌떼 습격'..학부모 \"아이 얼굴 퉁퉁\"(종합)\n",
      "https://v.daum.net/v/20210713154503726 : \"파지값 2만 8천원 때문에..\" 김해공항 미화원들 '대기발령'\n",
      "https://v.daum.net/v/20210713153813451 : '노도강' 찾은 윤석열, \"잘못된 임대차 3법으로 서민 고통 너무 크다\"\n",
      "https://v.daum.net/v/20210713133842565 : 정은경 '靑 방역기획관에 밀렸나' 질문에 \"소신껏 하고 있다\" 일축(종합)\n",
      "https://v.daum.net/v/20210713144944406 : 중등교원 양성 규모 축소..사범대 나와야 국영수 교사된다(종합)\n",
      "https://v.daum.net/v/20210713143600846 : 청와대 앞에 착륙한 미끼 전투기, 힘을 보여주고 싶었다\n",
      "https://v.daum.net/v/20210713131902974 : \"돼지코 같다\" 싸늘한 반응..BMW 4시리즈 참혹한 결과\n",
      "https://v.daum.net/v/20210713152226802 : 폭격 맞은 듯 순식간 '폭삭'..중국 1년만에 또 호텔붕괴 참사(종합)\n",
      "https://v.daum.net/v/20210713141248818 : \"제가 가르쳤고 믿고 있는..\" 조국, 아내 정경심 징역 7년 구형에 '무죄 추정' 대법 판결 공유\n",
      "https://v.daum.net/v/20210713152608926 : '지지율 1위' 여론조사 중단에 윤석열 캠프 \"외압\"-언론사 \"억측에 유감\"\n",
      "https://v.daum.net/v/20210713154101544 : '여름철 불청객' 대상포진..\"하루 20분 햇볕 쬐세요\"\n",
      "https://v.daum.net/v/20210713145535627 : '역시 접종이 답'..5월 60세 이상 확진자 94.7%, '미접종' 또는 '1차 14일 내 확진'\n",
      "https://v.daum.net/v/20210713164307759 : 식사 후 하면 건강에 '독' 되는 행동\n",
      "https://v.daum.net/v/20210713153227192 : [르포]박효신 '야생화' 트는 헬스장..\"신규회원 뚝, 축축 처진다\"\n",
      "https://v.daum.net/v/20210713153435278 : MBC '경찰 사칭' 옹호한 김의겸, 결국 고개 숙였다\n",
      "https://v.daum.net/v/20210713142508329 : 부산도 위태롭다..유흥시설 넘어 식당·학교 등 전방위 확산(종합)\n",
      "https://v.daum.net/v/20210713150102894 : [단독]수면부족·유축지옥에 지친 엄마들..\"도망가고 싶어요\"\n",
      "https://v.daum.net/v/20210713133325380 : 예고 없이 잡힌 미스터트롯 전주 콘서트, 잇단 항의에 취소\n",
      "https://v.daum.net/v/20210713153751429 : 베트남 박항서 축구대표 감독 산청군청 깜짝방문\n",
      "https://v.daum.net/v/20210713163604561 : 보호아동 종료 시기 18살→24살로 연장..'등 떠민 자립' 늦춘다\n",
      "https://v.daum.net/v/20210713114057051 : [단독]\"술집 못가니 호텔 가자\" 60대 유명화가, 20대 성폭행 의혹\n",
      "https://v.daum.net/v/20210713150915298 : 국내 접종 후 확진 '돌파감염' 총 252명..얀센 143명·화이자 59명·AZ 50명(상보)\n",
      "https://v.daum.net/v/20210713153049127 : [단독]함바왕, 사기죄 실형에 \"죽겠다\"..전자발찌 끊고 잠적\n",
      "https://v.daum.net/v/20210713143001565 : \"9월 전국민 70% 접종?..백신 없어서 불가능\"\n",
      "https://v.daum.net/v/20210713151017336 : 남자 더 뽑으려고 女 점수 낮춘 국민은행, '인사팀장' 항소심서 실형\n",
      "https://v.daum.net/v/20210713153349250 : \"산방산 온천 안 갔다니까\"..동선 숨긴 코로나 감염 제주 목사부부 '집유'\n",
      "https://v.daum.net/v/20210713140105375 : 연인 관계 2/3, 친구서 발전 (연구)\n",
      "https://v.daum.net/v/20210713153802440 : 中서 2명 살해 중국인, 신분세탁후 한국서 살다 34년만에 검거\n",
      "https://v.daum.net/v/20210713154459719 : 서울 시내 백화점 직원 코로나19 확진 잇따라\n",
      "https://v.daum.net/v/20210713153752430 : \"술집 못가니 호텔서 2차\" 60대 유명 화가, 20대 갤러리 직원 성폭행\n",
      "https://v.daum.net/v/20210713151603547 : 양어장을 탈출한 송어는 뇌가 커진다\n"
     ]
    }
   ],
   "source": [
    "# 다음 IT 뉴스 많이 본 뉴스\n",
    "# #mArticle > div.rank_news > ul.list_news2   div.cont_thumb > strong\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://news.daum.net/ranking/popular\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "news = soup.select(\"#mArticle > div.rank_news > ul.list_news2  div.cont_thumb > strong > a\")\n",
    "\n",
    "title = []\n",
    "j = 0\n",
    "for n in news:\n",
    "    title.append(n.get_text())\n",
    "    print(n['href'],':',title[j])\n",
    "    j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acabf797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0bafb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7f3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e4011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
